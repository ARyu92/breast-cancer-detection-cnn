{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07aae43d-098c-47c5-8f74-c4ba6ece54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tensors: 0 | Skipped: 0\n",
      "Train meta: 0 | Test meta: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import cv2\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydicom.pixel_data_handlers.util import apply_modality_lut, apply_voi_lut\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "data_directory = \"../data/Data/manifest/CBIS-DDSM/\"\n",
    "metadata_file  = \"../data/Data/manifest/all_data.csv\"\n",
    "\n",
    "train_dir = \"../data/Processed Data/train\"\n",
    "test_dir  = \"../data/Processed Data/test\"\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir,  exist_ok=True)\n",
    "\n",
    "TARGET_SIZE = (512, 512)\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def truncate_to_view_dir(p: str) -> str | None:\n",
    "    \"\"\"Return path up to and including the first CC/ or MLO/ directory (case-insensitive).\"\"\"\n",
    "    if not isinstance(p, str):\n",
    "        return None\n",
    "    p = p.replace(\"\\\\\", \"/\").strip()\n",
    "    m = re.search(r'/(CC|MLO)(/|$)', p, flags=re.I)\n",
    "    if not m:\n",
    "        return None\n",
    "    end = m.end()\n",
    "    out = p[:end]\n",
    "    if not out.endswith(\"/\"):\n",
    "        out = out + \"/\"\n",
    "    return out\n",
    "\n",
    "def get_dicom_files(directory_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a sorted list of .dcm files directly under 'directory_path' (non-recursive).\n",
    "    Accepts absolute path or relative to data_directory.\n",
    "    \"\"\"\n",
    "    cand1 = os.path.normpath(directory_path)\n",
    "    cand2 = os.path.normpath(os.path.join(data_directory, directory_path))\n",
    "    root = cand1 if os.path.isdir(cand1) else (cand2 if os.path.isdir(cand2) else None)\n",
    "    if root is None:\n",
    "        return []\n",
    "    files = [os.path.join(root, f) for f in os.listdir(root) if f.lower().endswith(\".dcm\")]\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def get_pixel_data(path: str, size=TARGET_SIZE) -> np.ndarray:\n",
    "    \"\"\"Read DICOM, apply Modality LUT -> VOI LUT, resize, return float32 array.\"\"\"\n",
    "    ds = pydicom.dcmread(path, force=True)\n",
    "    arr = apply_modality_lut(ds.pixel_array, ds)   # rescale slope/intercept etc.\n",
    "    arr = apply_voi_lut(arr, ds)                   # windowing\n",
    "    arr = arr.astype(np.float32, copy=False)\n",
    "    arr = cv2.resize(arr, size, interpolation=cv2.INTER_AREA)\n",
    "    return arr\n",
    "\n",
    "# --------------------\n",
    "# Load & prepare metadata\n",
    "# --------------------\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "\n",
    "# Robust label mapping\n",
    "label_mapping = {\"BENIGN\": 0, \"BENIGN_WITHOUT_CALLBACK\": 0, \"MALIGNANT\": 1}\n",
    "metadata[\"label\"] = (\n",
    "    metadata[\"pathology\"]\n",
    "    .astype(str).str.strip().str.upper()\n",
    "    .map(label_mapping)\n",
    ")\n",
    "\n",
    "# Normalize paths; build truncated CC/MLO dir path\n",
    "metadata[\"image file path\"] = (\n",
    "    metadata[\"image file path\"]\n",
    "    .astype(str).str.replace(\"\\\\\", \"/\", regex=False).str.strip()\n",
    ")\n",
    "metadata[\"truncated_path\"] = metadata[\"image file path\"].apply(truncate_to_view_dir)\n",
    "\n",
    "# Keep rows that matched CC/MLO and normalize view names\n",
    "metadata = metadata.dropna(subset=[\"truncated_path\"]).copy()\n",
    "metadata[\"image view\"] = metadata[\"image view\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# --------------------\n",
    "# Group and process\n",
    "# --------------------\n",
    "grouped = metadata.groupby([\"patient_id\", \"left or right breast\"])\n",
    "\n",
    "train_objects, test_objects = [], []\n",
    "saved, skipped = 0, 0\n",
    "\n",
    "for (patient_id, laterality), group in grouped:\n",
    "    views = group.set_index(\"image view\")\n",
    "\n",
    "    try:\n",
    "        # CC/MLO directories (handle multiple rows deterministically by taking first)\n",
    "        cc_path = views.loc[\"CC\", \"truncated_path\"]\n",
    "        if isinstance(cc_path, pd.Series):\n",
    "            cc_path = cc_path.iloc[0]\n",
    "        mlo_path = views.loc[\"MLO\", \"truncated_path\"]\n",
    "        if isinstance(mlo_path, pd.Series):\n",
    "            mlo_path = mlo_path.iloc[0]\n",
    "\n",
    "        label = int(views.iloc[0][\"label\"])\n",
    "\n",
    "        # Decide split from original paths (case-insensitive; default to train)\n",
    "        cc_low, mlo_low = cc_path.lower(), mlo_path.lower()\n",
    "        if \"test\" in cc_low or \"test\" in mlo_low:\n",
    "            save_dir, meta_list, split = test_dir, test_objects, \"test\"\n",
    "        else:\n",
    "            save_dir, meta_list, split = train_dir, train_objects, \"train\"\n",
    "\n",
    "        # Get first DICOM file in each CC/MLO dir (non-recursive)\n",
    "        cc_list = get_dicom_files(cc_path)\n",
    "        mlo_list = get_dicom_files(mlo_path)\n",
    "        if not cc_list or not mlo_list:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        cc_file = cc_list[0]\n",
    "        mlo_file = mlo_list[0]\n",
    "\n",
    "        # Load pixels and stack channels LAST: (H, W, 2)\n",
    "        cc_img  = get_pixel_data(cc_file)\n",
    "        mlo_img = get_pixel_data(mlo_file)\n",
    "        tensor = np.stack([cc_img, mlo_img], axis=-1).astype(np.float32)\n",
    "\n",
    "        # Save tensor\n",
    "        save_path = os.path.join(save_dir, f\"{patient_id}_{laterality}.npy\")\n",
    "        np.save(save_path, tensor)\n",
    "\n",
    "        # Record metadata\n",
    "        meta_list.append({\n",
    "            \"patient_id\": patient_id,\n",
    "            \"laterality\": laterality,\n",
    "            \"processed_path\": save_path,\n",
    "            \"label\": label,\n",
    "            \"split\": split\n",
    "        })\n",
    "        saved += 1\n",
    "\n",
    "    except Exception:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "# --------------------\n",
    "# Write meta files\n",
    "# --------------------\n",
    "with open(\"../data/meta_train.json\", \"w\") as f:\n",
    "    json.dump(train_objects, f)\n",
    "with open(\"../data/meta_test.json\", \"w\") as f:\n",
    "    json.dump(test_objects, f)\n",
    "\n",
    "print(f\"Saved tensors: {saved} | Skipped: {skipped}\")\n",
    "print(f\"Train meta: {len(train_objects)} | Test meta: {len(test_objects)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
